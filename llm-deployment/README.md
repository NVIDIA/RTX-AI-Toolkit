

This directory contains instructions to deploy the model generated by RTX AI Toolkit's fine-tuning process for deployment for the following workflows.


As seen in the LLM Finetuning tutorial, the LlamaFactory app supports export of either LoRA adapters or merged HF checkpoints. Here, we will see how to optimize both these options for deployment across a variety of platforms.

RTX AI Toolkit supports the following deployment workflows for the fine-tuned LLMs:

**Quantized (on-device) inference:**
| Platform | LoRA Adapter | Merged checkpoint |
| -------- | :------------------: | :-----------------------: |
| TensorRT-LLM | |  [✅](TensorRT-LLM_deployment.md) |
| llama.cpp |   |   [✅](llama.cpp_deployment.md) |
| ONNX Runtime - DML |     |   [✅](ORT-DML_QuickStart.md) |


**FP16 (cloud) inference:**
| Platform | LoRA Adapter | Merged checkpoint |
| -------- | :------------------: | :-----------------------: |
| vLLM |    [✅](vLLM_deployment.md#3-serving-lora-adapters) |   [✅](vLLM_deployment.md) |
| NIMs| ✅    |    |